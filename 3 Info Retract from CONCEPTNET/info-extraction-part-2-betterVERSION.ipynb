{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14291100,"sourceType":"datasetVersion","datasetId":9122238},{"sourceId":14300651,"sourceType":"datasetVersion","datasetId":9128853},{"sourceId":14731531,"sourceType":"datasetVersion","datasetId":9413692},{"sourceId":14754556,"sourceType":"datasetVersion","datasetId":9430312}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Cell 1 - installs & imports (run once)\n!pip install -q transformers sentence-transformers spacy protobuf==3.20.3\n!pip install -q torch torchvision torchaudio torch_geometric sentencepiece\n# (installing sentencepiece helps some models; adjust as needed)\n\nimport os\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n\nimport pandas as pd\nimport numpy as np\nimport spacy\nimport ast\nfrom typing import List, Dict, Tuple, Optional\nfrom sentence_transformers import SentenceTransformer, CrossEncoder\nfrom transformers import pipeline\nimport torch\nfrom tqdm.auto import tqdm\nimport pickle\n\nimport os, ast, re, json, pickle\nfrom tqdm.auto import tqdm\nfrom collections import defaultdict\nfrom typing import List, Tuple, Optional\n\nimport numpy as np\nimport pandas as pd\nimport torch\n\n# device\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device =\", DEVICE)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T17:44:04.558081Z","iopub.execute_input":"2026-02-04T17:44:04.559307Z","iopub.status.idle":"2026-02-04T17:44:12.299473Z","shell.execute_reply.started":"2026-02-04T17:44:04.559272Z","shell.execute_reply":"2026-02-04T17:44:12.298837Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import login\nlogin(\"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\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pairs_path = \"/kaggle/input/canndy/pairs_extracted.csv\"\nconceptnet_path = \"/kaggle/input/mintoc/conceptnet_df_clean.csv\"\n\npairs_df = pd.read_csv(pairs_path)\nprint(\"pairs_df shape:\", pairs_df.shape)\nprint(pairs_df.columns.tolist()[:20])\ndisplay(pairs_df.head(2))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"conceptnet_df = pd.read_csv(conceptnet_path, sep=\"\\t\", header=None)\nrequired_cols = [\"relation\", \"head\", \"tail\", \"weight\", \"edge_text\"]\nmissing = [c for c in required_cols if c not in conceptnet_df.columns]\n\nif missing:\n    raise ValueError(f\"Missing columns: {missing}\")\n\n# ensure string type\nfor col in [\"head\", \"tail\", \"relation\", \"edge_text\"]:\n    conceptnet_df[col] = conceptnet_df[col].astype(str)\n\n# ensure weight exists\nconceptnet_df[\"weight\"] = conceptnet_df[\"weight\"].fillna(1.0)\n\nprint(\"conceptnet_df:\", conceptnet_df.shape)\ndisplay(conceptnet_df.head(3))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# load edge embeddings from Kaggle dataset\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\n\nedge_embeddings = np.load(\n    \"/kaggle/input/edge-embedding/conceptnet_edge_embeddings (1).npy\"\n)\n\nprint(\"Loaded edge_embeddings. Shape:\", edge_embeddings.shape)\n\n# load bi-encoder (only needed later for queries, NOT for edges now)\nprint(\"Loading bi-encoder model...\")\nbi_encoder = SentenceTransformer(\"all-MiniLM-L6-v2\", device=DEVICE)\n\n# sanity check\nassert edge_embeddings.shape[0] == len(conceptnet_df), \"Mismatch with conceptnet_df rows\"\nprint(\"edge_embeddings OK. dim =\", edge_embeddings.shape[1])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 4 - cross-encoder for reranking (use CPU if CUDA not available)\ncross_encoder_model = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\ntry:\n    ce_device = DEVICE if DEVICE == \"cuda\" else \"cpu\"\n    cross_encoder = CrossEncoder(cross_encoder_model, device=ce_device)\n    print(\"Loaded CrossEncoder on\", ce_device)\nexcept Exception as e:\n    print(\"CrossEncoder load failed; falling back to CPU. Error:\", e)\n    cross_encoder = CrossEncoder(cross_encoder_model, device=\"cpu\")\n    print(\"CrossEncoder loaded on cpu\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 5 - small helpers to parse list-like fields\ndef ensure_list_field(x):\n    if pd.isna(x):\n        return []\n    if isinstance(x, list):\n        return x\n    if isinstance(x, (tuple, set)):\n        return list(x)\n    if isinstance(x, str):\n        try:\n            v = ast.literal_eval(x)\n            if isinstance(v, (list, tuple, set)):\n                return list(v)\n            # fallback: comma split\n            if \",\" in x:\n                return [s.strip() for s in x.split(\",\") if s.strip()]\n            return [x]\n        except Exception:\n            if \",\" in x:\n                return [s.strip() for s in x.split(\",\") if s.strip()]\n            return [x]\n    return [x]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"***Recall candidates (high recall, low precision)***","metadata":{}},{"cell_type":"code","source":"# Cell 6 - recall edges (uses bi_encoder for QA embedding and edge_embeddings for similarity)\ndef recall_edges(concepts, numbers, units, relations, qa_text,\n                 sim_threshold=0.05, recall_top_k=120):\n    # 1. concept filtering\n    concept_set = set([str(c) for c in concepts if c is not None])\n    mask = conceptnet_df[\"head\"].isin(concept_set) | conceptnet_df[\"tail\"].isin(concept_set)\n    candidate_idx = np.where(mask)[0]\n    if len(candidate_idx) == 0:\n        return []\n\n    # 2. enriched QA text\n    extra_signal = \" \".join(list(concepts) + list(relations) + [f\"{n} {u}\" for n, u in zip(numbers, units)])\n    enriched_qa_text = (qa_text or \"\") + \" \" + extra_signal\n\n    # 3. encode QA\n    qa_emb = bi_encoder.encode(enriched_qa_text, convert_to_numpy=True, normalize_embeddings=True).reshape(-1)\n\n    # 4. similarity (cosine because both normalized)\n    edge_embs = edge_embeddings[candidate_idx]\n    sims = np.dot(edge_embs, qa_emb)\n\n    # 5. soft prior by weight\n    weights = conceptnet_df.iloc[candidate_idx][\"weight\"].astype(float).values\n    sims = sims * (1.0 + 0.3 * np.log1p(weights))\n\n    # 6. filter & rank\n    keep_mask = sims >= sim_threshold\n    if keep_mask.sum() == 0:\n        return []\n    idx_kept = candidate_idx[keep_mask]\n    sims_kept = sims[keep_mask]\n    order = np.argsort(-sims_kept)[:recall_top_k]\n    results = [(int(idx_kept[i]), float(sims_kept[i])) for i in order]\n    return results\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 7 - cross-encoder reranker\ndef rerank_edges(qa_text: str,\n                 candidates: List[Tuple[int, float, Optional[List[str]]]],\n                 top_k: int = 40,\n                 batch_size: int = 32,\n                 include_path_in_prompt: bool = False,\n                 combine_with_prior: bool = True,\n                 prior_weight: float = 0.3):\n    if not candidates:\n        return []\n\n    # normalize priors\n    priors = [c[1] if len(c) > 1 and c[1] is not None else 0.0 for c in candidates]\n    max_prior = max(priors) if priors else 1.0\n    min_prior = min(priors) if priors else 0.0\n    def norm_prior(p):\n        if max_prior == min_prior:\n            return 0.0\n        return (p - min_prior) / (max_prior - min_prior)\n\n    texts = []\n    metas = []\n    for idx, prior, *rest in candidates:\n        path = rest[0] if rest else None\n        left = qa_text\n        if include_path_in_prompt and path:\n            left = qa_text + \" Context path: \" + \" -> \".join(path)\n        right = conceptnet_df.iloc[int(idx)][\"edge_text\"]\n        texts.append([left, right])\n        metas.append((int(idx), float(prior if prior is not None else 0.0), path))\n\n    scores = []\n    for i in range(0, len(texts), batch_size):\n        batch = texts[i:i+batch_size]\n        batch_scores = cross_encoder.predict(batch)  # returns floats\n        scores.extend(batch_scores)\n\n    results = []\n    for (idx, prior, path), cross_score in zip(metas, scores):\n        final_score = float(cross_score)\n        if combine_with_prior:\n            np_prior = norm_prior(prior)\n            final_score = final_score * (1.0 + prior_weight * np_prior)\n        results.append((idx, final_score, path))\n\n    results.sort(key=lambda x: -x[1])\n    return results[:top_k]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 8 - CoT disambiguation using an LLM pipeline if available.\n# If the LLM pipeline (llama) is not available or fails, we fall back to a simple lexical heuristic score.\n\nfrom transformers import pipeline\nLLM_AVAILABLE = False\nllm = None\ntry:\n    # try to create a small deterministic text-gen pipeline (user had Llama; this may not work in your env)\n    # If your environment has Llama-3 or other LLM, change this model name accordingly.\n    llm = pipeline(\"text-generation\", model=\"meta-llama/Llama-3.1-8B\", device_map=\"auto\", do_sample=False, max_new_tokens=120)\n    LLM_AVAILABLE = True\n    print(\"LLM pipeline created (CoT enabled).\")\nexcept Exception as e:\n    print(\"LLM not available in this environment, CoT will be skipped. Error:\", e)\n    LLM_AVAILABLE = False\n\ndef extract_json_from_text(text: str):\n    m = re.search(r\"\\{.*\\}\", text, flags=re.DOTALL)\n    if not m:\n        return None\n    s = m.group()\n    try:\n        return json.loads(s)\n    except Exception:\n        s2 = s.replace(\"'\", '\"')\n        s2 = re.sub(r\",\\s*}\", \"}\", s2)\n        s2 = re.sub(r\",\\s*]\", \"]\", s2)\n        try:\n            return json.loads(s2)\n        except:\n            return None\n\ndef cot_disambiguate(qa_text: str, edges: List[Tuple[int, float, Optional[List[str]]]], top_n: int = 10, temperature: float = 0.0):\n    if not edges:\n        return []\n    edges = edges[:top_n]\n    refined = []\n    if LLM_AVAILABLE:\n        for item in edges:\n            if len(item) >= 3:\n                idx, prior, path = item\n            else:\n                idx, prior = item\n                path = None\n            edge = conceptnet_df.iloc[int(idx)]\n            prompt = f\"\"\"Question & Answer:\n{qa_text}\n\nEdge:\n{edge['head']} {edge['relation']} {edge['tail']}\n\nTask: Decide whether the above edge logically supports the answer. Output ONLY a single JSON object with one key \"score\" (0.0-1.0).\nExample: {{\"score\": 0.75}}\nDo not add any other text.\n\"\"\"\n            out = llm(prompt, temperature=temperature, max_new_tokens=40, do_sample=False)[0][\"generated_text\"]\n            parsed = extract_json_from_text(out)\n            score = 0.0\n            if parsed and \"score\" in parsed:\n                try:\n                    score = float(parsed[\"score\"])\n                except:\n                    score = 0.0\n            else:\n                m = re.search(r\"([01](?:\\.\\d+)?)\", out)\n                if m:\n                    try:\n                        score = float(m.group(1))\n                    except:\n                        score = 0.0\n            score = max(0.0, min(1.0, score))\n            refined.append((int(idx), float(score)))\n    else:\n        # fallback: lexical overlap + normalized prior (fast heuristic)\n        for item in edges:\n            if len(item) >= 3:\n                idx, prior, path = item\n            else:\n                idx, prior = item\n            edge = conceptnet_df.iloc[int(idx)]\n            # count overlap tokens between QA and edge_text\n            qa_tokens = set(re.findall(r\"\\w+\", qa_text.lower()))\n            edge_tokens = set(re.findall(r\"\\w+\", edge[\"edge_text\"].lower()))\n            overlap = len(qa_tokens & edge_tokens) / max(1, len(edge_tokens))\n            # combine with prior (scaled)\n            score = 0.6 * overlap + 0.4 * (prior / (abs(prior) + 1.0))  # simple scaling\n            score = max(0.0, min(1.0, score))\n            refined.append((int(idx), float(score)))\n    refined.sort(key=lambda x: -x[1])\n    return refined\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 9 - main extraction function + loop through pairs_df, saving results for GCN later\nimport math\n\ndef extract_and_store_graph_edges(row,\n                                  sim_threshold=0.08,\n                                  recall_top_k=120,\n                                  rerank_top_k=50,\n                                  cot_top_n=15,\n                                  final_keep=40):\n    qa_text = (str(row.get(\"question\",\"\")) + \" \" + str(row.get(\"answer\",\"\"))).strip()\n    concepts = ensure_list_field(row.get(\"concepts\", []))\n    numbers = ensure_list_field(row.get(\"numbers\", []))\n    units = ensure_list_field(row.get(\"units\", []))\n    relations = ensure_list_field(row.get(\"relations\", []))\n\n    # 1. recall\n    recalled = recall_edges(concepts, numbers, units, relations, qa_text, sim_threshold=sim_threshold, recall_top_k=recall_top_k)\n    if not recalled:\n        return {\"display\": [], \"graph_edges\": []}\n\n    # 2. rerank\n    # rerank expects candidate list as (idx, prior, path_opt) â€” our recall returns (idx,score)\n    candidates = [(idx, score, None) for idx, score in recalled]\n    reranked = rerank_edges(qa_text, candidates, top_k=rerank_top_k)\n\n    # 3. If top two are close, use CoT / LLM to disambiguate\n    if len(reranked) > 1 and abs(reranked[0][1] - reranked[1][1]) < 0.05:\n        cot_edges = cot_disambiguate(qa_text, reranked, top_n=cot_top_n)\n        if cot_edges:\n            cot_dict = {idx: score for idx, score in cot_edges}\n            # replace reranked scores with cot scores where available\n            reranked = [(idx, float(cot_dict.get(idx, score)), *rest) for idx, score, *rest in reranked]\n\n    # 4. prune low supporting edges (keep top final_keep or those > threshold)\n    reranked_sorted = sorted(reranked, key=lambda x: -x[1])\n    pruned = reranked_sorted[:final_keep]\n\n    # build human display for top 3\n    final_display = pruned[:3]\n    display_list = [\n        {\"head\": conceptnet_df.iloc[idx][\"head\"],\n         \"relation\": conceptnet_df.iloc[idx][\"relation\"],\n         \"tail\": conceptnet_df.iloc[idx][\"tail\"],\n         \"score\": float(score)}\n        for idx, score, *rest in final_display\n    ]\n\n    # return both pruned graph edges and display\n    return {\"display\": display_list, \"graph_edges\": pruned}\n\n# Run the extraction loop with progress bar and save results\nresults = []\nfor i, row in tqdm(pairs_df.iterrows(), total=len(pairs_df)):\n    try:\n        res = extract_and_store_graph_edges(row, sim_threshold=0.08, recall_top_k=120, rerank_top_k=50, cot_top_n=15, final_keep=40)\n        results.append(res)\n    except Exception as e:\n        print(\"Row\", i, \"failed:\", e)\n        results.append({\"display\": [], \"graph_edges\": []})\n\npairs_df[\"cn_result\"] = results\n\n# Save results\npairs_df.to_csv(\"pairs_with_cn_results.csv\", index=False)\nwith open(\"pairs_conceptnet_graph_edges.pkl\", \"wb\") as f:\n    pickle.dump(pairs_df[\"cn_result\"].tolist(), f)\nprint(\"Saved pairs_with_cn_results.csv and pairs_conceptnet_graph_edges.pkl\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}